{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 호출\n",
    "data_path = \"../data\"\n",
    "train_df = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "sample_submission = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n",
    "\n",
    "# sub data\n",
    "interestrate_df = pd.read_csv(os.path.join(data_path, \"interestRate.csv\"))\n",
    "park_df = pd.read_csv(os.path.join(data_path, \"parkInfo.csv\"))\n",
    "school_df = pd.read_csv(os.path.join(data_path, \"schoolinfo.csv\"))\n",
    "subway_df = pd.read_csv(os.path.join(data_path, \"subwayInfo.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### park, school, subway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_dict = {'park':park_df, 'school':school_df, 'subway':subway_df}\n",
    "radii_dict = {\n",
    "    'park': [500, 1000, 1500, 2000],\n",
    "    'school': [100, 200, 300, 400, 500, 1000, 1500, 2000],\n",
    "    'subway': [300, 500, 800, 1000, 1500, 2000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# 위경도를 라디안으로 변환\n",
    "def to_radians(df, lat_col='latitude', lon_col='longitude'):\n",
    "    df['latitude_radi'] = np.radians(df[lat_col])\n",
    "    df['longitude_radi'] = np.radians(df[lon_col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_places(main_df, places_dict, radii_dict):\n",
    "    \"\"\"\n",
    "    main_df: 원본 데이터 (계산 대상 위치)\n",
    "    places_dict: 장소 데이터 딕셔너리, {장소 이름: 장소 데이터프레임}\n",
    "    radii_dict: 반경 리스트 딕셔너리, {장소 이름: 반경 리스트(예: [300, 500, 1000])}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 위경도를 라디안으로 변환 (main_df에 적용)\n",
    "    main_df = to_radians(main_df)\n",
    "\n",
    "    # 각 장소 유형별로 BallTree를 생성하여 반경 내 개수를 계산\n",
    "    for place_name, add_df in places_dict.items():\n",
    "\n",
    "        # 각 장소 데이터의 위경도를 라디안으로 변환\n",
    "        add_df = to_radians(add_df)\n",
    "\n",
    "        # 장소 데이터에 대해 BallTree 생성\n",
    "        ball_tree = BallTree(add_df[['latitude_radi', 'longitude_radi']].values, metric='haversine')\n",
    "\n",
    "        for radius in radii_dict[place_name]:\n",
    "            # 반경을 km로 변환\n",
    "            radius_in_km = radius / 1000\n",
    "\n",
    "            # 반경 내 place 개수 계산(반경을 지구 반지름(6371km)로 나눈 값)\n",
    "            _, indices = ball_tree.query_radius(main_df[['latitude_radi', 'longitude_radi']].values, r=radius_in_km/6371, return_distance=True)\n",
    "\n",
    "            # 반경별 장소 개수 열 추가\n",
    "            main_df[f'{place_name}_{radius}m'] = [len(idx) for idx in indices]\n",
    "        \n",
    "        distances, distances_index = ball_tree.query(main_df[['latitude_radi', 'longitude_radi']].values, k=1)\n",
    "        main_df[f'{place_name}_near_distance'] = distances.flatten()*6371\n",
    "\n",
    "        # 공원: area 추가\n",
    "        if place_name == 'park':\n",
    "            main_df['park_near_area'] = add_df.iloc[distances_index.flatten(), 2].reset_index(drop=True)\n",
    "\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = count_places(train_df, places_dict, radii_dict)\n",
    "train_df.to_csv(\"train_df_seperate.csv\")\n",
    "test_df = count_places(test_df, places_dict, radii_dict)\n",
    "test_df.to_csv(\"test_df_seperate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_df.csv\")\n",
    "test_df = pd.read_csv(\"test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['age'] = train_df['age'].clip(lower=0)\n",
    "\n",
    "age_mapping = {'0-10': 0, '10-20': 10, '20-30': 20, '30+': 30}\n",
    "train_df['age_group'] = pd.cut(train_df['age'], bins=[0, 10, 20, 30, 300], labels=[0, 10, 20, 30], right=False)\n",
    "train_df['age_group'] = train_df['age_group'].astype(int)\n",
    "test_df['age_group'] = pd.cut(test_df['age'], bins=[0, 10, 20, 30, 300], labels=[0, 10, 20, 30], right=False)\n",
    "test_df['age_group'] = test_df['age_group'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['index', 'contract_day', 'age'])\n",
    "test_df = test_df.drop(columns=['index', 'contract_day', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['contract_type'] = train_df['contract_type'].replace(2, np.nan)\n",
    "test_df['contract_type'] = test_df['contract_type'].replace(2, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add feature : deposit per area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['deposit_per_area'] = train_df['deposit'] / train_df['area_m2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_copy = train_df\n",
    "\n",
    "y_valid_real = train_df_copy[(train_df_copy['contract_year_month'] >= 202307) & (train_df_copy['contract_year_month'] <= 202312)]\n",
    "\n",
    "y_valid_real_deposit = y_valid_real['deposit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['deposit'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_start = 202307\n",
    "holdout_end = 202312\n",
    "valid_df = train_df[(train_df['contract_year_month'] >= holdout_start) & (train_df['contract_year_month'] <= holdout_end)]\n",
    "final_train_df = train_df[~((train_df['contract_year_month'] >= holdout_start) & (train_df['contract_year_month'] <= holdout_end))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X, y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = final_train_df.drop(columns=['deposit_per_area'])\n",
    "y_train = final_train_df['deposit_per_area']\n",
    "X_valid = valid_df.drop(columns=['deposit_per_area'])\n",
    "y_valid = valid_df['deposit_per_area']\n",
    "X_test = test_df.copy()\n",
    "\n",
    "X_total = train_df.drop(columns=['deposit_per_area'])\n",
    "y_total = train_df['deposit_per_area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsymmetricHuberLoss:\n",
    "    def __init__(self, delta=1.0, beta=1.05):   # 작게 예측하는 값에 대해 loss를 키운다.(default : 5%)\n",
    "        self.delta = delta\n",
    "        self.beta = beta\n",
    "\n",
    "    def _calculate_loss(self, y_true, y_pred):\n",
    "\n",
    "        # 식 구현\n",
    "        error = y_true - y_pred\n",
    "        abs_error = np.abs(error) / y_true\n",
    "        quadratic = np.minimum(abs_error, self.delta)   \n",
    "        linear = abs_error - quadratic \n",
    "        loss = 0.5 * quadratic**2 + self.delta * linear\n",
    "\n",
    "        # loss penalty 추가\n",
    "        underestimation_mask = y_pred < y_true * 0.95 \n",
    "        loss[underestimation_mask] *= self.beta\n",
    "        return loss   # eval_metric에 사용\n",
    "\n",
    "    def gradient(self, y_true, y_pred):   # 1차 미분값\n",
    "        error = y_pred - y_true\n",
    "        abs_error = np.abs(error) / y_true\n",
    "        grad = np.where(abs_error <= self.delta, error, self.delta * np.sign(error))  \n",
    "        grad[y_pred < y_true * 0.95] *= self.beta   \n",
    "        return grad\n",
    "\n",
    "    def hessian(self, y_true, y_pred):   # 2차 미분값\n",
    "        abs_error = np.abs(y_pred - y_true) / y_true\n",
    "        hess = np.where(abs_error <= self.delta, 1.0, 0.0)\n",
    "        hess[y_pred < y_true * 0.95] *= self.beta   \n",
    "        return hess\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    loss = AsymmetricHuberLoss()\n",
    "    grad = loss.gradient(y_true, y_pred)\n",
    "    hess = loss.hessian(y_true, y_pred)\n",
    "    return grad, hess\n",
    "\n",
    "def custom_metric(y_pred, y_true):\n",
    "    loss = AsymmetricHuberLoss()\n",
    "    return np.mean(loss._calculate_loss(y_true, y_pred)) * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 군집화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/house/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# train + valid 데이터로 최적의 k 찾기\n",
    "best_k = 10\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_SEED)\n",
    "kmeans.fit(X_total[['latitude', 'longitude']])\n",
    "total_pred = kmeans.predict(X_total[['latitude', 'longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 5, 5, 5], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터에 대한 cluster 예측\n",
    "test_pred = kmeans.predict(X_test[['latitude', 'longitude']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_models = []\n",
    "best_iterations = []\n",
    "train_pred = kmeans.predict(X_train[['latitude', 'longitude']])\n",
    "valid_pred = kmeans.predict(X_valid[['latitude', 'longitude']])\n",
    "X_train = X_train.drop(columns=['latitude', 'longitude'])\n",
    "X_valid = X_valid.drop(columns=['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 modeling...\n",
      "[0]\tvalidation_0-rmse:531.25830\tvalidation_0-custom_metric:48911.09085\n",
      "[50]\tvalidation_0-rmse:225.61841\tvalidation_0-custom_metric:233.06107\n",
      "[100]\tvalidation_0-rmse:121.61232\tvalidation_0-custom_metric:37.64549\n",
      "[150]\tvalidation_0-rmse:90.26400\tvalidation_0-custom_metric:17.91112\n",
      "[200]\tvalidation_0-rmse:81.28798\tvalidation_0-custom_metric:13.98423\n",
      "[250]\tvalidation_0-rmse:78.58293\tvalidation_0-custom_metric:12.90064\n",
      "[300]\tvalidation_0-rmse:77.77264\tvalidation_0-custom_metric:12.56116\n",
      "[350]\tvalidation_0-rmse:77.48353\tvalidation_0-custom_metric:12.46947\n",
      "[373]\tvalidation_0-rmse:77.47522\tvalidation_0-custom_metric:12.49817\n",
      "Cluster 1 modeling...\n",
      "[0]\tvalidation_0-rmse:518.57477\tvalidation_0-custom_metric:49467.11731\n",
      "[50]\tvalidation_0-rmse:220.86951\tvalidation_0-custom_metric:248.38427\n",
      "[100]\tvalidation_0-rmse:117.75989\tvalidation_0-custom_metric:39.87058\n",
      "[150]\tvalidation_0-rmse:84.41791\tvalidation_0-custom_metric:18.00197\n",
      "[200]\tvalidation_0-rmse:74.36652\tvalidation_0-custom_metric:13.48295\n",
      "[250]\tvalidation_0-rmse:70.91190\tvalidation_0-custom_metric:12.06154\n",
      "[300]\tvalidation_0-rmse:69.67269\tvalidation_0-custom_metric:11.59420\n",
      "[350]\tvalidation_0-rmse:69.09924\tvalidation_0-custom_metric:11.39285\n",
      "[400]\tvalidation_0-rmse:68.88828\tvalidation_0-custom_metric:11.32875\n",
      "[450]\tvalidation_0-rmse:68.79679\tvalidation_0-custom_metric:11.29315\n",
      "[491]\tvalidation_0-rmse:68.76116\tvalidation_0-custom_metric:11.28362\n",
      "Cluster 2 modeling...\n",
      "[0]\tvalidation_0-rmse:364.42331\tvalidation_0-custom_metric:47609.51233\n",
      "[50]\tvalidation_0-rmse:152.92615\tvalidation_0-custom_metric:228.90212\n",
      "[100]\tvalidation_0-rmse:80.40549\tvalidation_0-custom_metric:34.81885\n",
      "[150]\tvalidation_0-rmse:57.73292\tvalidation_0-custom_metric:15.32302\n",
      "[200]\tvalidation_0-rmse:50.98752\tvalidation_0-custom_metric:11.43443\n",
      "[250]\tvalidation_0-rmse:48.86425\tvalidation_0-custom_metric:10.38331\n",
      "[300]\tvalidation_0-rmse:48.15190\tvalidation_0-custom_metric:10.07489\n",
      "[350]\tvalidation_0-rmse:47.85308\tvalidation_0-custom_metric:9.95429\n",
      "[400]\tvalidation_0-rmse:47.73665\tvalidation_0-custom_metric:9.92788\n",
      "[403]\tvalidation_0-rmse:47.73373\tvalidation_0-custom_metric:9.92787\n",
      "Cluster 3 modeling...\n",
      "[0]\tvalidation_0-rmse:455.22413\tvalidation_0-custom_metric:46173.99597\n",
      "[50]\tvalidation_0-rmse:182.61745\tvalidation_0-custom_metric:199.14824\n",
      "[100]\tvalidation_0-rmse:93.02292\tvalidation_0-custom_metric:29.63509\n",
      "[150]\tvalidation_0-rmse:67.85209\tvalidation_0-custom_metric:14.19062\n",
      "[200]\tvalidation_0-rmse:61.75344\tvalidation_0-custom_metric:11.45294\n",
      "[250]\tvalidation_0-rmse:59.93235\tvalidation_0-custom_metric:10.70409\n",
      "[300]\tvalidation_0-rmse:59.46045\tvalidation_0-custom_metric:10.50074\n",
      "[350]\tvalidation_0-rmse:59.27275\tvalidation_0-custom_metric:10.43096\n",
      "[400]\tvalidation_0-rmse:59.25039\tvalidation_0-custom_metric:10.41503\n",
      "[404]\tvalidation_0-rmse:59.26339\tvalidation_0-custom_metric:10.41659\n",
      "Cluster 4 modeling...\n",
      "[0]\tvalidation_0-rmse:283.82267\tvalidation_0-custom_metric:45990.48615\n",
      "[50]\tvalidation_0-rmse:115.08468\tvalidation_0-custom_metric:201.74125\n",
      "[100]\tvalidation_0-rmse:58.50848\tvalidation_0-custom_metric:28.32255\n",
      "[150]\tvalidation_0-rmse:42.06267\tvalidation_0-custom_metric:12.42791\n",
      "[200]\tvalidation_0-rmse:37.46186\tvalidation_0-custom_metric:9.40386\n",
      "[250]\tvalidation_0-rmse:36.20955\tvalidation_0-custom_metric:8.66845\n",
      "[300]\tvalidation_0-rmse:35.80484\tvalidation_0-custom_metric:8.47659\n",
      "[350]\tvalidation_0-rmse:35.70028\tvalidation_0-custom_metric:8.44392\n",
      "[362]\tvalidation_0-rmse:35.70408\tvalidation_0-custom_metric:8.45786\n",
      "Cluster 5 modeling...\n",
      "[0]\tvalidation_0-rmse:356.30499\tvalidation_0-custom_metric:47783.64563\n",
      "[50]\tvalidation_0-rmse:152.99917\tvalidation_0-custom_metric:242.66322\n",
      "[100]\tvalidation_0-rmse:83.47876\tvalidation_0-custom_metric:40.15412\n",
      "[150]\tvalidation_0-rmse:61.74221\tvalidation_0-custom_metric:18.85258\n",
      "[200]\tvalidation_0-rmse:54.90204\tvalidation_0-custom_metric:14.39896\n",
      "[250]\tvalidation_0-rmse:52.50388\tvalidation_0-custom_metric:13.03346\n",
      "[300]\tvalidation_0-rmse:51.55309\tvalidation_0-custom_metric:12.55261\n",
      "[350]\tvalidation_0-rmse:51.13397\tvalidation_0-custom_metric:12.35759\n",
      "[400]\tvalidation_0-rmse:51.00316\tvalidation_0-custom_metric:12.30437\n",
      "[450]\tvalidation_0-rmse:50.91237\tvalidation_0-custom_metric:12.27542\n",
      "[499]\tvalidation_0-rmse:50.86949\tvalidation_0-custom_metric:12.26253\n",
      "Cluster 6 modeling...\n",
      "[0]\tvalidation_0-rmse:931.84483\tvalidation_0-custom_metric:49215.48080\n",
      "[50]\tvalidation_0-rmse:398.52621\tvalidation_0-custom_metric:232.96103\n",
      "[100]\tvalidation_0-rmse:225.38260\tvalidation_0-custom_metric:39.50033\n",
      "[150]\tvalidation_0-rmse:173.37072\tvalidation_0-custom_metric:18.85073\n",
      "[200]\tvalidation_0-rmse:157.69991\tvalidation_0-custom_metric:14.67482\n",
      "[250]\tvalidation_0-rmse:152.75254\tvalidation_0-custom_metric:13.50849\n",
      "[300]\tvalidation_0-rmse:151.02537\tvalidation_0-custom_metric:13.13759\n",
      "[350]\tvalidation_0-rmse:150.40958\tvalidation_0-custom_metric:13.06197\n",
      "[400]\tvalidation_0-rmse:150.01392\tvalidation_0-custom_metric:13.00800\n",
      "[450]\tvalidation_0-rmse:149.82653\tvalidation_0-custom_metric:12.98840\n",
      "[484]\tvalidation_0-rmse:149.80368\tvalidation_0-custom_metric:12.98645\n",
      "Cluster 7 modeling...\n",
      "[0]\tvalidation_0-rmse:671.75670\tvalidation_0-custom_metric:47959.43451\n",
      "[50]\tvalidation_0-rmse:283.48584\tvalidation_0-custom_metric:221.64492\n",
      "[100]\tvalidation_0-rmse:156.31264\tvalidation_0-custom_metric:37.56431\n",
      "[150]\tvalidation_0-rmse:117.65258\tvalidation_0-custom_metric:18.84361\n",
      "[200]\tvalidation_0-rmse:105.17832\tvalidation_0-custom_metric:14.78308\n",
      "[250]\tvalidation_0-rmse:100.63001\tvalidation_0-custom_metric:13.52026\n",
      "[300]\tvalidation_0-rmse:98.66995\tvalidation_0-custom_metric:12.99416\n",
      "[350]\tvalidation_0-rmse:97.76626\tvalidation_0-custom_metric:12.75990\n",
      "[400]\tvalidation_0-rmse:97.30632\tvalidation_0-custom_metric:12.64427\n",
      "[450]\tvalidation_0-rmse:97.10189\tvalidation_0-custom_metric:12.60019\n",
      "[499]\tvalidation_0-rmse:96.94397\tvalidation_0-custom_metric:12.56728\n",
      "Cluster 8 modeling...\n",
      "[0]\tvalidation_0-rmse:470.24662\tvalidation_0-custom_metric:47532.02820\n",
      "[50]\tvalidation_0-rmse:195.31798\tvalidation_0-custom_metric:218.53566\n",
      "[100]\tvalidation_0-rmse:101.78635\tvalidation_0-custom_metric:33.66244\n",
      "[150]\tvalidation_0-rmse:72.73682\tvalidation_0-custom_metric:15.23802\n",
      "[200]\tvalidation_0-rmse:64.39055\tvalidation_0-custom_metric:11.78392\n",
      "[250]\tvalidation_0-rmse:61.69841\tvalidation_0-custom_metric:10.82172\n",
      "[300]\tvalidation_0-rmse:60.92146\tvalidation_0-custom_metric:10.56700\n",
      "[350]\tvalidation_0-rmse:60.53036\tvalidation_0-custom_metric:10.42024\n",
      "[400]\tvalidation_0-rmse:60.38348\tvalidation_0-custom_metric:10.37371\n",
      "[450]\tvalidation_0-rmse:60.35236\tvalidation_0-custom_metric:10.37051\n",
      "[465]\tvalidation_0-rmse:60.35861\tvalidation_0-custom_metric:10.37583\n",
      "Cluster 9 modeling...\n",
      "[0]\tvalidation_0-rmse:284.00635\tvalidation_0-custom_metric:44116.01639\n",
      "[50]\tvalidation_0-rmse:112.66651\tvalidation_0-custom_metric:189.27826\n",
      "[100]\tvalidation_0-rmse:59.32503\tvalidation_0-custom_metric:31.25008\n",
      "[150]\tvalidation_0-rmse:45.49560\tvalidation_0-custom_metric:16.30498\n",
      "[200]\tvalidation_0-rmse:41.87535\tvalidation_0-custom_metric:13.35247\n",
      "[250]\tvalidation_0-rmse:40.89571\tvalidation_0-custom_metric:12.55978\n",
      "[300]\tvalidation_0-rmse:40.52834\tvalidation_0-custom_metric:12.31005\n",
      "[319]\tvalidation_0-rmse:40.51211\tvalidation_0-custom_metric:12.31240\n"
     ]
    }
   ],
   "source": [
    "for i in range(best_k):\n",
    "    print(f'Cluster {i} modeling...')\n",
    "    train_cluster_idx = np.where(train_pred == i)[0]   # (index_array, dtype)\n",
    "    valid_cluster_idx = np.where(valid_pred == i)[0]\n",
    "\n",
    "    X_train_cluster = X_train.iloc[train_cluster_idx]\n",
    "    y_train_cluster = y_train.iloc[train_cluster_idx]\n",
    "\n",
    "    X_valid_cluster = X_valid.iloc[valid_cluster_idx]\n",
    "    y_valid_cluster = y_valid.iloc[valid_cluster_idx]\n",
    "\n",
    "    xgb_params = {\n",
    "        'objective': custom_loss,   # default : reg:squarederror\n",
    "        'eval_metric': custom_metric,   # default : rmse\n",
    "        'seed': RANDOM_SEED,\n",
    "        'n_estimators': 500,\n",
    "        'learning_rate': 0.02,   # default : 0.3\n",
    "        'max_depth': 12,\n",
    "        # 'subsample': 0.9,\n",
    "        # 'colsample_bytree': 0.9,\n",
    "        # 'reg_alpha': 10.0,\n",
    "        # 'reg_lambda': 10.0,\n",
    "        'early_stopping_rounds':20,\n",
    "        'n_jobs': -1,\n",
    "        'device': 'cuda'\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "    xgb_model.fit(X_train_cluster, y_train_cluster, eval_set=[(X_valid_cluster, y_valid_cluster)], verbose=50)\n",
    "    best_iterations.append(xgb_model.best_iteration)\n",
    "\n",
    "    xgb_models.append(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xgb_model in xgb_models:\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plot_importance(xgb_model, importance_type='gain')  # max_num_features로 출력할 feature 개수 지정 가능\n",
    "#     plt.title('Feature Importance')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 206866 entries, 774291 to 1801227\n",
      "Series name: deposit\n",
      "Non-Null Count   Dtype  \n",
      "--------------   -----  \n",
      "206866 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "y_valid_real_deposit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/house/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [01:21:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4240.15151856872\n"
     ]
    }
   ],
   "source": [
    "X_valid['pred'] = 0\n",
    "for i in range(best_k):\n",
    "    valid_cluster_idx = np.where(valid_pred == i)[0]\n",
    "    X_valid_cluster = X_valid.iloc[valid_cluster_idx]\n",
    "    X_valid.loc[X_valid_cluster.index, 'pred'] = xgb_models[i].predict(X_valid_cluster.drop(columns=['pred']))\n",
    "\n",
    "valid_pred = X_valid['pred'] * X_valid['area_m2']\n",
    "valid_mae = mean_absolute_error(y_valid_real_deposit, valid_pred)\n",
    "\n",
    "print(valid_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 modeling...\n",
      "Cluster 1 modeling...\n",
      "Cluster 2 modeling...\n",
      "Cluster 3 modeling...\n",
      "Cluster 4 modeling...\n",
      "Cluster 5 modeling...\n",
      "Cluster 6 modeling...\n",
      "Cluster 7 modeling...\n",
      "Cluster 8 modeling...\n",
      "Cluster 9 modeling...\n"
     ]
    }
   ],
   "source": [
    "xgb_models = []\n",
    "total_pred = kmeans.predict(X_total[['latitude', 'longitude']])\n",
    "X_total = X_total.drop(columns=['latitude', 'longitude'])\n",
    "for i in range(best_k):\n",
    "    print(f'Cluster {i} modeling...')\n",
    "    total_cluster_idx = np.where(total_pred == i)[0]   # (index_array, dtype)\n",
    "\n",
    "    X_total_cluster = X_total.iloc[total_cluster_idx]\n",
    "    y_total_cluster = y_total.iloc[total_cluster_idx]\n",
    "\n",
    "    xgb_params = {\n",
    "        'objective': custom_loss,\n",
    "        'eval_metric': custom_metric,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'n_estimators': round(best_iterations[i], -1),   # 1의 자리에서 반올림\n",
    "        'learning_rate': 0.02,   # default : 0.3\n",
    "        'max_depth': 12,\n",
    "        # 'subsample': 0.9,\n",
    "        # 'colsample_bytree': 0.9,\n",
    "        # 'reg_alpha': 10.0,\n",
    "        # 'reg_lambda': 10.0,\n",
    "        # 'early_stopping_rounds':20,\n",
    "        'n_jobs': -1,\n",
    "        'device': 'cuda'\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "    xgb_model.fit(X_total_cluster, y_total_cluster, verbose=20)\n",
    "\n",
    "    xgb_models.append(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['pred'] = 0\n",
    "X_test = X_test.drop(columns=['latitude', 'longitude'])\n",
    "for i in range(best_k):\n",
    "    test_cluster_idx = np.where(test_pred == i)[0]\n",
    "    X_test_cluster = X_test.iloc[test_cluster_idx]\n",
    "    X_test.loc[X_test_cluster.index, 'pred'] = xgb_models[i].predict(X_test_cluster.drop(columns=['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_xgb_cluster = X_test['pred'] * X_test['area_m2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['deposit'] = test_pred_xgb_cluster\n",
    "sample_submission.to_csv('output.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
